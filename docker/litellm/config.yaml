# Models are managed via the UI and stored in the database
# This config file contains only infrastructure settings
#
# To add Ollama models via the UI (https://admin.your-domain.com/ui):
# 1. Pull models in Ollama: docker exec chatbridge-ollama ollama pull llama2
# 2. Add model in LiteLLM UI with:
#    - Model Name: llama2 (or any name)
#    - LiteLLM Params:
#        model: ollama/llama2
#        api_base: http://ollama:11434
#
# Popular Ollama models to try:
# - ollama/llama2, ollama/llama3, ollama/mistral, ollama/codellama
# - ollama/phi, ollama/gemma, ollama/qwen

# Router Settings
router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
  enable_pre_call_checks: true
  # Fallbacks are managed via the UI

# General Settings
litellm_settings:
  drop_params: true
  set_verbose: false
  json_logs: true
  
  # Caching
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    password: os.environ/REDIS_PASSWORD
    ttl: 3600
  
  # Rate Limiting
  num_retries: 3
  request_timeout: 300

  # Content Moderation (disabled - configure separately if needed)
  enable_content_moderation: false

# Success/Failure Callbacks (add monitoring services to enable)
# success_callback: ["langfuse", "prometheus"]
# failure_callback: ["langfuse", "prometheus"]

# General Settings for all models
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

  # Budgets and Rate Limits
  max_budget: 100.0
  budget_duration: "1d"

  # Logging
  store_model_in_db: true

  # UI Settings
  ui_access_mode: "all"
  ui_username: os.environ/UI_USERNAME
  ui_password: os.environ/UI_PASSWORD
